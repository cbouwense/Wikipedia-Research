{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Christian Bouwense\n",
    "\n",
    "Program that gets the revision for a page by user for a certain time interval and measures burstiness.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import random\n",
    "import datetime as dt\n",
    "import mwapi\n",
    "import operator\n",
    "import numpy as np\n",
    "import dateutil.parser as dup\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_sample() creates a sample of data based on the constraints given to it; this way I was able to test out my code which implemented Barabasi's formulas to measure burstiness. You can create different sizes of interevent times, and guage how often they should switch (change the effects of memory of the system). You do this by setting the length and probability of short interevent times occuring, and the same for long interevent times.\n",
    "\n",
    "The function takes seven arguments, but they are pretty simple to understand.\n",
    "\n",
    "n: Sample size\n",
    "small_lower: Lower bound of short interevent times.\n",
    "small_upper: Upper bound of short interevent times.\n",
    "large_lower: Lower bound of long interevent times.\n",
    "large_upper: Upper bound of long interevent times.\n",
    "stop_short: Probability that, after a short interevent time, there will be a long interevent time. [0, 1]\n",
    "stop_long: Probability that, after a long interevent time, there will be a short interevent time. [0, 1]\n",
    "print_results: Boolean which defaults to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(continue_short, continue_long, n=10000, small_lower=0, small_upper=0.1, large_lower=500, large_upper=600):\n",
    "    just_did_short = True\n",
    "    # Initiate with a small interevent time \n",
    "    sample = [small_lower + np.mean([small_lower, small_upper])]\n",
    "\n",
    "    for i in range(1, n):\n",
    "        # The random number generate to check if we change type of interevent time (long to short or short to long)\n",
    "        change_chance = random.random()\n",
    "        # If we just did a short interevent time and we rolled a number in the probability to stay, make another small.\n",
    "        # Or, if we just did a long time but we did not roll another long, make a small.\n",
    "        if (just_did_short and change_chance <= continue_short) or (not just_did_short and change_chance > continue_long):\n",
    "            just_did_short = True\n",
    "            sample.append(random.uniform(small_lower, small_upper))\n",
    "        else:\n",
    "            just_did_short = False\n",
    "            sample.append(random.uniform(large_lower, large_upper))\n",
    "            \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates sample of data that is bursty due to memory\n",
    "bursty_sample = generate_sample(0.99, 0.99)\n",
    "# Generates sample of data that is not bursty\n",
    "non_bursty_sample = generate_sample(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the arguments we are going to pass to the MediaWiki API. It specifies that we want to query the article titled \"Game of Thrones\" for revisions. We ask for the timestamp and user of each revision. The maximum amount of revisions any lay user is allowed per request is 500, so we request the \"max\" amount (in order to get all the revisions we just make a bunch of queries until we have all the revisions). We also specify that we only want the revisions made between two certain dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Information specifying article and time interval to look at\n",
    "article_title = 'Game of Thrones'\n",
    "\n",
    "today = dt.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "action = 'query'\n",
    "prop = 'revisions'\n",
    "rv_prop = 'timestamp|user'\n",
    "rv_limit = 'max'\n",
    "rv_start = today\n",
    "rv_end = '2010-08-05T00:00:00Z'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mwapi.Session connects to the English Wikipedia database, and we send a GET REST request using the mwapi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to Wikipedia\n",
    "session = mwapi.Session('https://en.wikipedia.org', user_agent='cbouwense')\n",
    "\n",
    "# Query Wikipedia for revisions on the supplied article\n",
    "# The result is stored into the dictionary \"rev_dict\"\n",
    "rev_dict = session.get(action=action,\n",
    "                       prop=prop,\n",
    "                       rvprop=rv_prop,\n",
    "                       titles=article_title,\n",
    "                       rvlimit=rv_limit,\n",
    "                       rvstart=today,\n",
    "                       rvend=rv_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the revisions made by each user in a dictionary: the keys will be each user and the values will be the amount of revisions each user made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "revisions_by_user = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little weird, but necessary. We need the pageID of the article we are analyzing to use when stripping the revisions out of the JSON it gives us. If that didn't make sense it will later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find page_id for selected article\n",
    "for keys in rev_dict['query']['pages'].keys():\n",
    "    page_id = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the timestamps for each revision made.\n",
    "# If the timestamp is already a key in our dictionary, increment that key value by 1.\n",
    "# Else, create a new key for that year in our dictionary and set it to 1\n",
    "rev_timestamps = []\n",
    "for props in rev_dict['query']['pages'][str(page_id)]['revisions']:\n",
    "    if (props['user'] not in revisions_by_user):\n",
    "        revisions_by_user[props['user']] = 1\n",
    "    else:\n",
    "        revisions_by_user[props['user']] += 1\n",
    "\n",
    "    timestamp = dup.parse(props['timestamp'])\n",
    "    rev_timestamps.append(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the maximum revisions a user can get from a single query is 500. If there are more than 500 revisions for the query, then there will be a key in the JSON named \"continue\" and \"rvcontinue.\" We can use the value of \"rvcontinue\" to pick up where we left off and get revisions 501-1000 (assuming there are that many). We do this until all the revisions are received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data from Wikipedia.\n",
      "This could take up to a few minutes depending on the article...\n"
     ]
    }
   ],
   "source": [
    "# Check if there is a section named \"continue\".\n",
    "# If there is, that means the query did not get all the data\n",
    "# because of the per-user query limits.\n",
    "print (\"Retrieving data from Wikipedia.\")\n",
    "print (\"This could take up to a few minutes depending on the article...\")\n",
    "while 'continue' in rev_dict:\n",
    "    continue_val = rev_dict['continue']['rvcontinue']\n",
    "    rev_dict = session.get(action=action,\n",
    "                           prop=prop,\n",
    "                           rvprop=rv_prop,\n",
    "                           titles=article_title,\n",
    "                           rvlimit=rv_limit,\n",
    "                           rvstart=today,\n",
    "                           rvend=rv_end,\n",
    "                           rvcontinue=continue_val)\n",
    "    for props in rev_dict['query']['pages'][str(page_id)]['revisions']:\n",
    "        if 'user' in props:\n",
    "            if (props['user'] not in revisions_by_user):\n",
    "                revisions_by_user[props['user']] = 1\n",
    "            else:\n",
    "                revisions_by_user[props['user']] += 1\n",
    "        timestamp = dup.parse(props['timestamp'])\n",
    "        rev_timestamps.append(timestamp)\n",
    "        \n",
    "# List of tuples of revisions made by user for page\n",
    "sorted_user_revisions = sorted(revisions_by_user.items(), key=operator.itemgetter(1))[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the ways of measuring burstiness is analyzing the distribution of the time between events in the system (interevent times). In order to do this, we must enumerate all the interevent times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enumerate the times between events into a list\n",
    "# Also calculate the summation for M while you're at it\n",
    "interevent_times = []\n",
    "summation = 0\n",
    "for i in range(0, len(rev_timestamps)-1):\n",
    "    interevent_times.append((rev_timestamps[i] - rev_timestamps[i+1]).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the interevent times, we need to find their mean and standard deviation. Luckily for us, the Python module Numpy has functions for that built in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate interevent time mean and standard deviation\n",
    "interevent_mean = (np.mean(interevent_times))\n",
    "interevent_std_dev = (np.std(interevent_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to use the mean and standard deviation. The measure of burstiness by interevent times is defined as follows.\n",
    "$$B \\ \\triangleq\\ \\frac{\\sigma_{\\tau}-m_{\\tau}}{\\sigma_{\\tau}+m_{\\tau}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B = 0.433187253169\n"
     ]
    }
   ],
   "source": [
    "B = ((interevent_std_dev - interevent_mean) / (interevent_std_dev + interevent_mean))\n",
    "print (\"B = %s\" % B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of B ranges from -1 to 1: the closer to -1 the more regular the events occur, closer to 0 means the closer they are to being completely random, and the closer to 1 the more bursty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a different angle that we can measure burstiness with: memory. At first it may sound the same but it is actually completely independent of interevent time distribution. Memory means that short interevent times are really likely to happen after short ones, and long ones occur after long ones.  \n",
    "\n",
    "In order to measure this, we will need the mean and standard deviation of interevent times [1, n-1], and [2, n] (assuming a total of n interevent times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1 = np.mean(interevent_times[0:len(interevent_times)-1])\n",
    "mean_2 = np.mean(interevent_times[1:len(interevent_times)])\n",
    "std_dev_1 = np.std(interevent_times[0:len(interevent_times)-1])\n",
    "std_dev_2 = np.std(interevent_times[1:len(interevent_times)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of burstiness due to memory is defined as\n",
    "\n",
    "$$M \\ \\triangleq\\ \\frac{1}{n_{\\tau} - 1}\\sum_{i=1}^{n_{\\tau}-1} \\frac{(\\tau_{i}-m_{1})(\\tau_{i+1}-m_{2})}{\\sigma_{1}\\sigma_{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 0.100900969388\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(interevent_times)-1):\n",
    "    tau_i = interevent_times[i]\n",
    "    tau_i_plus_one = interevent_times[i+1]\n",
    "    summation_term = (((tau_i - mean_1) * (tau_i_plus_one - mean_2)) / (std_dev_1 * std_dev_2)) \n",
    "    summation += summation_term\n",
    "\n",
    "M = (1 / (len(interevent_times) - 1)) * summation \n",
    "\n",
    "print (\"M = %s\" % M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
