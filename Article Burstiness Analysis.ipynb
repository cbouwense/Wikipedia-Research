{"metadata":{"language_info":{"name":"python","version":"3.6.1","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":2,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Wikipedia Edits: Burstiness Analysis\nChristian Bouwense\n\n## Introduction\nThe world can be thought of as a set of systems. These consist of components and interactions between them. Think of two people friending each other on Facebook, or internet routers exchanging packets of information. On the surface, these systems may seem very different, but the underlying principle of actors and actions is the same.\n\nIf we can understand the behavior of these systems - what they have in common and what they do not - we could react to our world in a more efficient way. We could have an algorithm to allocate FEMA funds more accurately based on Hurricane patterns before disasters, or know when to administer what type of treatment based on cancer metabolization patterns.\n\n## What is Burstiness?\nThis is not a new idea. Mathematicians have tried to model systems for decades. However, classical statistics have not given very good results. The general idea is that human interaction is random, and if that is the case we should be able to model interactions with random statistical models. Unfortunately, the world is not that simple, and many different systems have very dissimilar behaviors.\n\nFortunately, there is a new way of modelling system behavior: burstiness. Burstiness is a measure of how regular, random, or \"bursty\" interactions in an system are. If system behavior is \"bursty,\" there are long periods of inactivity, followed by short bursts of activity. On the opposite side of the spectrum, systems that are very regularly timed interactions are \"anti-bursty,\" such as time between heartbeats at rest. The middle of the scale is completely random behavior, such as the time inbetween winning a slot machine.\n\n## Methodology\nWe can do better than just describing burstiness in English; this paper by Barabasi introduced the idea of burstiness, and provides mathematical ways of measuring it.\n\nFor this experiment, we will study the system of Wikipedia article editing. This is a very interesting case because it involves human and non-human interaction (bots can make edits as well), over many different subjects of varying degrees of public interest. They also have a ton of data.\n\nIn order to measure the burstiness of Wikipedia editing (I'll be using \"edits\" and \"revisions\" interchangeably), we need a program to download the data, and then measure its burstiness. For this I'll be using Python, notably using the mwapi module, which lets you query Wikipedia's databases.\n\n## The Code\nAbove every code cell, I will be putting descriptions of what the code is doing, and how the code is doing it. If you do not understand Python code, then reading these descriptions should give you a good idea of what's going on. If you are interested in the nitty-gritty of the Python, then reading the \"how\" should give you some insight. (I have marked the high level explanations with \"Description\" and nitty-gritty explanations with \"Technical\" to save people time!)\n\n## Importing Modules\nTechnical: Here we just import some modules that we will need in the program. The most notable one, as I have mentioned earlier, is the inclusion of \"mwapi\" which is an interfact between Python and REST APIs which we can send to Wikipedia. An interesting point is that mwapi is actually a wrapper for another module called mw, where you need to actually craft the REST calls yourself.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAuthor: Christian Bouwense\n\nProgram that gets the revision for a page by user for a certain time \ninterval and measures burstiness.\n\"\"\"\n\nimport time\nimport random\nimport datetime as dt\nimport mwapi\nimport operator\nimport numpy as np\nimport dateutil.parser as dup\nfrom matplotlib import pyplot as plt\nimport matplotlib.patches as mpatches","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Getting the Data\n\n**Description**: Here I am creating a bit of code that will ask Wikipedia for the revision data of any article we want. You can specify which article you want data for, a time period, and what kind of data you want about the revisions (e.g. who made the revisions, when the revisions was made, etc.).\n\n**Technical**: get_revisions() is a wrapper for the mwapi get() call. We are always going to want to query, get revisions, and accept the maximum amount of revisions. However, you can also specify things like the revision properties you want, and the date range you wish to see (and if you don't really care, they have default values). The values are returned in JSON and stored in a global dictionary called article_data. So even though we are getting data, this function has a void return type.\n\nYou may notice that there is a for loop that assigns a variable called page_id. We will need this value to traverse the JSON later when we parse its contents, and also for continuing the query. Speaking about continuing the query, any get request is only allowed 500 objects to be received per call. However, if there are more than 500 revisions, there will be a field in the JSON called 'continue.' If you put the value of 'continue' into your next get call, it will return you the next 500. So we have a while loop that checks for this continue value, and if its there just continually make get calls until its gone.","metadata":{}},{"cell_type":"code","source":"def get_revisions(title, rv_prop='timestamp|user', rv_start='today', rv_end='2000-01-01T00:00:00Z'):\n    # Information specifying article and time interval to look at\n    title = title\n    \n    # We're always going to want these parameters to be the same\n    action = 'query'\n    prop = 'revisions'\n    rv_limit = 'max'\n    \n    today = dt.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%dT%H:%M:%SZ')\n    # User can just give the string \"today\" instead of the timestamp\n    if rv_start == \"today\":\n        rv_start = today\n    else:\n        rv_start = start_date\n    \n    # Temporary dictionary holding amount of revisions for each user\n    revisions_by_user = {}\n    \n    # Connect to Wikipedia\n    session = mwapi.Session('https://en.wikipedia.org', user_agent='cbouwense')\n\n    # Query Wikipedia for revisions on the supplied article\n    # The result is stored into the dictionary \"rev_dict\"\n    rev_dict = session.get(action=action,\n                           prop=prop,\n                           rvprop=rv_prop,\n                           titles=title,\n                           rvlimit=rv_limit,\n                           rvstart=rv_start,\n                           rvend=rv_end)\n    \n    # Find page_id for selected article\n    for keys in rev_dict['query']['pages'].keys():\n        page_id = keys\n        \n    # Go through the timestamps for each revision made.\n    # If the timestamp is already a key in our dictionary, increment that key value by 1.\n    # Else, create a new key for that year in our dictionary and set it to 1\n    rev_timestamps = []\n    for props in rev_dict['query']['pages'][str(page_id)]['revisions']:\n        if 'user' in props:\n            if (props['user'] not in revisions_by_user):\n                revisions_by_user[props['user']] = 1\n            else:\n                revisions_by_user[props['user']] += 1\n\n        timestamp = dup.parse(props['timestamp'])\n        rev_timestamps.append(timestamp)\n        \n    # Check if there is a section named \"continue\".\n    # If there is, that means the query did not get all the data\n    # because of the per-user query limits.\n    print (\"Retrieving data on %s from Wikipedia...\" % title)\n    while 'continue' in rev_dict:\n        continue_val = rev_dict['continue']['rvcontinue']\n        rev_dict = session.get(action=action,\n                               prop=prop,\n                               rvprop=rv_prop,\n                               titles=title,\n                               rvlimit=rv_limit,\n                               rvstart=today,\n                               rvend=rv_end,\n                               rvcontinue=continue_val)\n        for props in rev_dict['query']['pages'][str(page_id)]['revisions']:\n            if 'user' in props:\n                if (props['user'] not in revisions_by_user):\n                    revisions_by_user[props['user']] = 1\n                else:\n                    revisions_by_user[props['user']] += 1\n            timestamp = dup.parse(props['timestamp'])\n            rev_timestamps.append(timestamp)\n\n    # List of tuples of revisions made by user for page\n    sorted_user_revisions = sorted(revisions_by_user.items(), key=operator.itemgetter(1))[::-1]\n    \n    # Enumerate the times between events into a list\n    interevent_times = []\n    for i in range(0, len(rev_timestamps)-1):\n        interevent_times.append((rev_timestamps[i] - rev_timestamps[i+1]).total_seconds())\n    \n    # Add data to global dictionaries\n    article_data[title] = {}\n    article_data[title]['user_revs'] = sorted_user_revisions\n    article_data[title]['revision_times'] = rev_timestamps\n    article_data[title]['interevent_times'] = interevent_times\n    get_B(article)\n    get_M(article)\n    \n    print (\"Data received successfully!\")","metadata":{},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Storing Article Data\n**Description**: We need a place to store all of this data that Wikipedia is giving us, so this one line of code below is the place we will keep it. When we ask Wikipedia for the revision data of an article, we will put the amount of edits per user, the times each revision was created, and how much time is inbetween revisions in article_data.\n\n**Technical**: This is simply a global dictionary to store the data parsed by get_revisions(). It has a key for each article we query. Each article has a key for 'user_revs' (a list of tuples containing a username and how many edits they created), 'revision_times' (a list of timestamps for each edit), and 'interevent_times' (a list of floats indicating the amount of seconds between each edit).","metadata":{}},{"cell_type":"code","source":"article_data = {}","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Analyzing Burstiness: B\n**Description**: Now that we have a way of getting data from Wikipedia, we need to create a way to analyze its burstiness. One of the ways of measuring burstiness is looking at the distribution of the time between events in the system (interevent times). In order to do this, we need to find their mean and standard deviation and plug them into one of the two burstiness equations.\nThe measure of burstiness by interevent times is defined as follows:\n\n$$B \\ \\triangleq\\ \\frac{\\sigma_{\\tau}-m_{\\tau}}{\\sigma_{\\tau}+m_{\\tau}}$$\n\nIf you're not a math person, essentially we are just taking some facts about the data and plugging them into an equation. You can think of this equation as a \"Burstiness-o-meter.\" If we take some data and put it into the \"Burstiness-o-meter\" it spits out a number. This output is simply a number from -1 to 1; let's call it B. The lower the number is, the less bursty it is, and the higher it is, the burstier. If it is close to 0, that means that the data is random.\nTechnical: We use the Numpy module's built-in functions to find the mean and standard deviation of the data. Since we already stored the interevent times in the article_data, we can just give those to the Numpy functions.\nB is essentially analyzing how different the distribution of interevent times is from the Poisson Distribution. The math here is pretty simple, we're just taking the difference between the standard deviation and the mean over the sum of the standard deviation and the mean. The range of this formula is \\[1, -1\\]. The $\\lim_{\\sigma\\to 0} B=-1$, which makes sense: as the data deviates less and less, there are less \"bursty\" situations. As $B\\to0$ the distribution of interevent times more closely resembles the Poisson distribution, meaning that they are random. Finally as $B\\to1$, the burstier the data.","metadata":{}},{"cell_type":"code","source":"def get_B(title):\n    # Generate probability distribution for data (credits to Nico!)\n    hist, bins_e = np.histogram(article_data[title]['interevent_times'], bins=100, density=True)\n    # Calculate interevent time mean and standard deviation\n    interevent_mean = (np.mean(hist))\n    interevent_std_dev = (np.std(hist))\n    B = ((interevent_std_dev - interevent_mean) / (interevent_std_dev + interevent_mean))\n    article_data[title]['B'] = B","metadata":{},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_M(title):\n    # Store times in this variable with a much shorter name\n    times = article_data[title]['interevent_times']\n    \n    mean_1 = np.mean(times[0:len(times)-1])\n    mean_2 = np.mean(times[1:len(times)])\n    std_dev_1 = np.std(times[0:len(times)-1])\n    std_dev_2 = np.std(times[1:len(times)])\n    \n    summation = 0\n    for i in range(0, len(times)-1):\n        tau_i = times[i]\n        tau_i_plus_one = times[i+1]\n        summation_term = (((tau_i - mean_1) * (tau_i_plus_one - mean_2)) / (std_dev_1 * std_dev_2)) \n        summation += summation_term\n\n    M = (1 / (len(times) - 1)) * summation\n    \n    article_data[title]['M'] = M","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"articles = [\n'George W. Bush',\n'List of WWE personnel',\n'United States',\n'Wikipedia',\n'Michael Jackson',\n'Jesus',\n'Catholic Church',\n'List of programs broadcast by ABS-CBN',\n'Donald Trump',\n'Barack Obama',\n'Adolf Hitler',\n'Britney Spears',\n'World War II',\n'The Beatles',\n'India',\n'Game of Thrones'\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Getting data for 15 articles.\")\nprint (\"This may take a while!\")\nfor article in articles:\n    get_revisions(article)\n    print ('B: %s' % article_data[article]['B'])\n    print ('M: %s' % article_data[article]['M'])","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","text":"Getting data for 15 articles.\nThis may take a while!\nRetrieving data on George W. Bush from Wikipedia...\nData received successfully!\nB: 0.809861836243\nM: 0.249184340631\nRetrieving data on List of WWE personnel from Wikipedia...\nData received successfully!\nB: 0.766413803831\nM: 0.198896239552\nRetrieving data on United States from Wikipedia...\nData received successfully!\nB: 0.806829854521\nM: 0.34232454673\nRetrieving data on Wikipedia from Wikipedia...\nData received successfully!\nB: 0.799837012974\nM: 0.193968133666\nRetrieving data on Michael Jackson from Wikipedia...\nData received successfully!\nB: 0.811588352226\nM: 0.415830004457\nRetrieving data on Jesus from Wikipedia...\nData received successfully!\nB: 0.815187206911\nM: 0.191254880132\nRetrieving data on Catholic Church from Wikipedia...\n","output_type":"stream"}]},{"cell_type":"code","source":"ind = np.arange(len(article_data.keys()))\nB_vals = []\nM_vals = []\nfor article in article_data.keys():\n    B_vals.append(article_data[article]['B'])\n    M_vals.append(article_data[article]['M'])\n    \nwidth = 0.4\nfig, ax = plt.subplots()\nplt.title('B & M of Article Inter-Revision Times', fontsize=26)\nplt.yticks(ind, article_data.keys(), rotation=0, fontsize=18)\nplt.xticks(fontsize=18)\nplt.xlim(-1, 1)\nplt.xlabel('Burstiness', fontsize=20)\n\nax.barh(ind, B_vals, width, color='#FF9933', label='N')\nax.barh(ind + width, M_vals, width, color='#0099CC', label='M')\nfor i in range(-10, 10):\n    ax.axvline(i/10, linestyle='dotted', zorder=0)\n    \n\norange_patch = mpatches.Patch(color='#FF9933', label='B')\nblue_patch = mpatches.Patch(color='#0099CC', label='M')\nplt.legend(handles=[orange_patch, blue_patch])\n\nfig.set_size_inches(10, 10)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind = np.arange(len(article_data.keys()))\narticle_editors = []\narticle_edits = []\n\narticle_count = 0\nfor article in article_data.keys():\n    article_editors.append(0)\n    article_edits.append(0)\n    for tuple in article_data[article]['user_revs']:\n        article_editors[article_count] += 1\n        article_edits[article_count] += tuple[1]\n    article_count += 1\n    \nfig, ax = plt.subplots()\nplt.title('Edit Stats for Articles', fontsize=26)\nplt.yticks(ind, article_data.keys(), rotation=0, fontsize=18)\nplt.xticks(fontsize=18)\n\nax.barh(ind, article_editors, width, color='#990033', label='Editors')\nax.barh(ind + width, article_edits, width, color='#669900', label='Edits')    \n\n#for i in range(0, max(article_edits)):\n#    ax.axvline(i, linestyle='dotted', zorder=0)\n    #i += max(article_edits) / 10\n\nred_patch = mpatches.Patch(color='#990033', label='Editors')\ngreen_patch = mpatches.Patch(color='#669900', label='Edits')\nplt.legend(handles=[red_patch, green_patch])\n\nfig.set_size_inches(10, 10)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}